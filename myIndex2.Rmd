---
name: Daniel Kirkland
title: "WebScrapingFun"
output:
  html_document:
    df_print: paged
---

## Introduction
Let's attempt to scrape some data from the CSU Chico course scheduling website.  

```{r, results='hide', message=FALSE}
include <- function(library_name){
  if( !(library_name %in% installed.packages()) )
    install.packages(library_name) 
  library(library_name, character.only=TRUE)
}
include("rvest")
include("tidyr")
```

First, let's load the html link for the Spring 2019 semester:
```{r}
my_url1 <- read_html("http://ems.csuchico.edu/APSS/schedule/spr2019/CSCI.shtml")
 

```
On each of the pages whose links are imported, there exists a table which communicates information about the courses being taught by both the Math and Computer Science departments at CSU Chico.  The observation information of interest is: 

*Subj*, *Cat Num*, *Sect*, *Title*, *Instructor*, *Tot Enrl* 

First, let's import data for the Computer Science department's course schedule for Spring 2019.  After successfully importing the data and creating a tibble from it, we can explore the creation of a function capable of importing data from multiple websites and storing such data in a single tibble.   

```{r}
##Spring 2019 courses taught
#******(Note: we can use 'copy selector' from the 'inspect element' data to obtain the exact name needed for aliasing. Ex. #maintable uses # to indicate 'id=____', vs 'td._____'td.Instructor)
courseListingSP19 <- my_url1 %>% html_nodes("#maintable")

###Scraping the Subj data
subj <- courseListingSP19 %>% 
              html_nodes("td.subj") %>% 
              html_text()  

###Scraping the Cat Num data
cat_num <- courseListingSP19 %>% 
              html_nodes("td.cat_num") %>% 
              html_text() %>%
              as.integer()

###Scraping the Sect data
section <- courseListingSP19 %>% 
              html_nodes("td.sect") %>% 
              html_text()  

###Scraping the Title data
course_title <- courseListingSP19 %>% 
              html_nodes("td.title") %>% 
              html_text()  

###Scraping the Instructor data
instructor_name <- courseListingSP19 %>% 
                    html_nodes("td.Instructor") %>% 
                    html_text()

###Scraping the Tot Enrl data
tot_enrolled <- courseListingSP19 %>% 
          html_nodes("td.enrtot") %>% 
          html_text() %>%
          as.integer()

###Creating a tibbles for the Spring 2019 semester
spring2019classes <- tibble(Subject= subj, Category_Number= cat_num, Section_Number= section, Course_Title= course_title, Instructor= instructor_name, Enrollment= tot_enrolled, Term= "Spring 2019")

spring2019classes

```


Now let's try to create a function to automate the process for importing urls and scraping the desired data.  

```{r}
    read_class_schedule <- function(url, term){
      my_url <- read_html(url)
      course <- my_url %>% html_nodes("#maintable")

      ###Scraping the Subj data
      subj <- course %>% 
                    html_nodes("td.subj") %>% 
                    html_text()  
      
      ###Scraping the Cat Num data
      cat_num <- course %>% 
                    html_nodes("td.cat_num") %>% 
                    html_text() %>%
                    as.integer()
      
      ###Scraping the Sect data
      section <- course %>% 
                    html_nodes("td.sect") %>% 
                    html_text()  
      
      ###Scraping the Title data
      course_title <- course %>% 
                    html_nodes("td.title") %>% 
                    html_text()  
      
      ###Scraping the Instructor data
      instructor_name <- course %>% 
                          html_nodes("td.Instructor") %>% 
                          html_text()
      
      ###Scraping the Tot Enrl data
      tot_enrolled <- course %>% 
                html_nodes("td.enrtot") %>% 
                html_text() %>%
                as.integer()
      
      ###Creating a tibble for the current semester
      class_schedule <- tibble(Subject= subj, Category_Number= cat_num, Section_Number= section, Course_Title= course_title, Instructor= instructor_name, Enrollment= tot_enrolled, Term= term)
      return(class_schedule)
    }

```
Now the the function is written, let's try to import data from the urls for the Computer Science department's Spring 19 and Spring 20, and the Math department's Spring 19 and Spring 20 semesters.  


```{r}
  Spring_2019_CS <- read_class_schedule("http://ems.csuchico.edu/APSS/schedule/spr2019/CSCI.shtml", "Spring 2019 CS")

  Spring_2019_CS
  
  
  Spring_2020_CS <- read_class_schedule("http://ems.csuchico.edu/APSS/schedule/spr2020/CSCI.shtml", "Spring 2020 CS")

  Spring_2020_CS
  
  Spring_2019_MATH <- read_class_schedule("http://ems.csuchico.edu/APSS/schedule/spr2019/MATH.shtml", "Spring 2019 MATH")

  Spring_2019_MATH
  
  
  Spring_2020_MATH <- read_class_schedule("http://ems.csuchico.edu/APSS/schedule/spr2020/MATH.shtml", "Spring 2020 MATH")

  Spring_2020_MATH
  
```

Now that all of the desired data has been scraped and stored into tibbles respective to each department/semester, let's combine all of the tibbles into one single tibble using a full join. 

```{r}
install.packages("dplyr")
library(dplyr)
course_schedules <- 
full_join(Spring_2019_CS,  Spring_2020_CS, by = NULL)

course_schedules <- full_join(course_schedules,  Spring_2019_MATH, by = NULL)

course_schedules <- full_join(course_schedules, Spring_2020_MATH, by = NULL)

course_schedules


```

Now, all tibbles have been combined into a single tibble, "course_schedules". 



